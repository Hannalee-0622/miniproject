{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a857b1d7",
   "metadata": {},
   "source": [
    "### 가상 환경 활성화\n",
    "\n",
    "1. 원하는 폴더로 이동  \n",
    "```cd C:\\Users\\user-name\\your-directory-path```\n",
    "  \n",
    "<br>  \n",
    "\n",
    "2. 가상환경 생성  \n",
    "```python -m venv huggingface-env```\n",
    "  \n",
    "<br>  \n",
    "\n",
    "3. 가상환경 활성화   \n",
    "```.\\huggingface-env\\Scripts\\activate```\n",
    "\n",
    "<br>  \n",
    ">  활성화되면 프롬프트가 (huggingface-env)로 바뀝니다.\n",
    "\n",
    "<br>  \n",
    "\n",
    "+) 가상환경 비활성화:  \n",
    "```deactivate```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592977d2",
   "metadata": {},
   "source": [
    "### requirements.txt. 설치\n",
    "\n",
    "가상 환경이 활성화가 되었으면 다음 명령어로 모듈 설치:  \n",
    "\n",
    "```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "만약, 에러가 발생한다면 pip 최신화 먼저 실행:  \n",
    "```python -m pip install --upgrade pip```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e1eaa",
   "metadata": {},
   "source": [
    "## Hugging Face 간단한 예제\n",
    "\n",
    "1. Hugging Face와 Gradio를 이용해서 간단한 질의응답을 할 수 있는 간단한 웹앱을 실행해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e490f0",
   "metadata": {},
   "source": [
    "1. transformer: BERE, GPT, TS 등 유명한 모델을 사용할 수 있음\n",
    "2. 사전학습 모델 허브: 연구자들이 학습시킨 모델을 공유하눈 플랫폼\n",
    "3. 손쉬운 테스트: 파이썬 코드로 대형 AI 모델을 사용가능\n",
    "4. 다양한 테스크 지원: 거의 모든 NLP 작업 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb60e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment=analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd3d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannalee/Documents/코딩/huggingface-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9976332783699036, 'start': 0, 'end': 5, 'answer': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", force_download=True)\n",
    "result = qa(question=\"What is the capital of France?\", context=\"Paris is the capital of France\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e494fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannalee/Documents/코딩/huggingface-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", force_download=True)\n",
    "\n",
    "def answer_question(context, question):\n",
    "    result = qa_pipeline({\n",
    "        'context' : context,\n",
    "        'question' : question\n",
    "    })\n",
    "    return result['answer']\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, label=\"Context (문맥 입력)\", placeholder=\"여기에 문맥을 입력하세요.\"),\n",
    "        gr.Textbox(lines=1, label=\"Question (질문)\", placeholder=\"질문을 입력하세요.\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Answer (답변)\"),\n",
    "    title=\"🤗 Hugging Face Q&A 데모\",\n",
    "    description=\"Hugging Face의 Transformers 모델을 이용한 문맥 기반 질의응답 웹앱\"\n",
    ")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb12907",
   "metadata": {},
   "source": [
    "## DDPG 다차원 공간 학습 및 시뮬레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dc608",
   "metadata": {},
   "source": [
    "1. **BipedalWalker-v3**는 연속 행동 공간 중 대표적인 복잡한 환경입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecca9a0",
   "metadata": {},
   "source": [
    "| 항목               | Pendulum-v1  | BipedalWalker-v3 |\n",
    "| ---------------- | ------------ | ---------------- |\n",
    "| 상태 공간 (`state`)  | 3차원          | 24차원             |\n",
    "| 행동 공간 (`action`) | 1차원 (1개 토크)  | 4차원 (4개 다리 제어)   |\n",
    "| 보상 구조            | 수직 유지를 위한 보상 | 목표까지 걷는 성과 기반    |\n",
    "| 종료 조건            | 없음           | 넘어진 경우 조기 종료 가능  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccb888",
   "metadata": {},
   "source": [
    "2. 학습을 위한 라이브러리들을 import 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806e6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663e623",
   "metadata": {},
   "source": [
    "### 환경 불러오기\n",
    "\n",
    "1. BipedalWalker-v3 환경을 불러온뒤, 학습에 필요한 차원들을 정의하는 코드를 작성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6490969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54faf1",
   "metadata": {},
   "source": [
    "### 모델 구조 작성\n",
    "\n",
    "1. BipedalWalker-v3 환경에 적합한 Actor 신경망을 PyTorch로 정의하세요.  \n",
    "\n",
    "(힌트: 상태 → 행동 출력 / 연속 공간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f52bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.out = nn.Linear(300, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return torch.tanh(self.out(x)) * max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fbc81",
   "metadata": {},
   "source": [
    "2. Critic 네트워크를 정의하세요. 입력은 상태와 행동을 받아 Q-value를 예측해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2ba292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.out = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = torch.cat([x, u], 1) # 1 == dim == 열방향 == 가로로\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c972d0",
   "metadata": {},
   "source": [
    "### Replay Buffer 구현\n",
    "\n",
    "1. 다음 조건을 만족하는 ReplayBuffer 클래스를 직접 작성하세요.\n",
    "\n",
    "- 경험 데이터를 저장할 수 있어야 함\n",
    "\n",
    "- 저장된 경험에서 무작위로 일정량을 샘플링할 수 있어야 함\n",
    "\n",
    "- 저장된 전체 데이터의 개수를 반환할 수 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2771eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size = 1000000):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)  # (state, action, reward, next_state, done) = (s, a, r, s', True)\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*samples))\n",
    "        return (\n",
    "            torch.FloatTensor(state).to(device),\n",
    "            torch.FloatTensor(action).to(device),\n",
    "            torch.FloatTensor(reward).unsqueeze(1).to(device),\n",
    "            torch.FloatTensor(next_state).to(device),\n",
    "            torch.FloatTensor(done).unsqueeze(1).to(device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e69c46",
   "metadata": {},
   "source": [
    "### DDPG 신경망과 하이퍼파라미터 초기화\n",
    "\n",
    "1. DDPG에서 신경망과 하이퍼파라미터를 초기화하는 코드를 작성하세요.  \n",
    "다음 조건을 만족해야 합니다:\n",
    "\n",
    "- Actor, Critic, Target Actor, Target Critic 네트워크를 정의하고 GPU 또는 CPU에 할당하세요.\n",
    "\n",
    "- Target 네트워크는 각각 학습 초기 네트워크의 파라미터로 동기화되어야 합니다.\n",
    "\n",
    "- Actor와 Critic에 대해 각각 다른 학습률을 갖는 Adam 옵티마이저를 생성하세요.\n",
    "\n",
    "- 경험을 저장할 수 있는 ReplayBuffer 인스턴스를 생성하세요.\n",
    "\n",
    "- 할인율(gamma), 타겟 소프트 업데이트 계수(tau), 배치 크기(batch_size), 탐험 노이즈(exploration_noise)를 정의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c76e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = Actor().to(device)\n",
    "critic = Critic().to(device)\n",
    "target_actor = Actor().to(device)\n",
    "target_critic = Critic().to(device)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "replay_buffer = ReplayBuffer()\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 64\n",
    "exploration_noise = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f224994",
   "metadata": {},
   "source": [
    "2. DDPG에서 Actor와 Critic을 학습시키는 train() 함수를 작성하세요.  \n",
    "\n",
    "다음 조건을 충족해야 합니다:\n",
    "\n",
    "- Replay Buffer에서 일정 수 이상의 데이터가 쌓였을 때만 학습이 시작되도록 하세요.\n",
    "\n",
    "- Critic 네트워크는 타겟 Q값을 사용해 MSE Loss를 기준으로 학습하세요.\n",
    "\n",
    "- Actor 네트워크는 Critic이 높게 평가하는 방향으로 정책을 업데이트하세요.\n",
    "\n",
    "- 학습 후, **타겟 네트워크(actor/critic)** 를 Soft Update 방식으로 업데이트하세요.\n",
    "\n",
    "- Critic Loss는 ```old_critic_losses``` 리스트에, Episode Reward는 외부에서 ```old_episode_rewards```에 저장된다고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da9994d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "critic_losses = []\n",
    "def train():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    with torch.no_grad():\n",
    "        target_action = target_actor(next_state)\n",
    "        target_q = reward + (1 - done) * gamma * target_critic(next_state, target_action)\n",
    "    currnet_q = critic(state, action)\n",
    "    critic_loss = F.mse_loss(currnet_q, target_q)\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    critic_losses.append(critic_loss.item())\n",
    "    actor_loss = -critic(state, actor(state)).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "    for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc614733",
   "metadata": {},
   "source": [
    "3. DDPG 학습 루프를 작성하고 학습을 진행해보세요.\n",
    "\n",
    "각 에피소드마다 얻은 보상을 누적하여 ```old_episode_reward```로 저장하고, 이를 ```old_episode_rewards``` 리스트에 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e789567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2fbef4",
   "metadata": {},
   "source": [
    "3. DDPG 학습 결과를 시각화하고 모델을 저장하세요. \n",
    "\n",
    "그래프의 제목은 각각 \"Episode Rewards\", \"Critic Losses\"로 지정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d73b490f",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정\n",
    "\n",
    "1. 학습 결과 보상이 ```-100 ~ 0``` 사이에 있습니다. 유의미한 학습이 되지 않았다는 뜻으로 하이퍼파라미터를 조정하여 다시 학습해보겠습니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "**noise**는 ```0.1 -> 0.2```로 증가, **episode**는 ```1000~2000``` 사이로 조정, **timestep**은 ```1600``` 이하로 설정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68305d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40573a8f",
   "metadata": {},
   "source": [
    "2. 변경된 하이퍼파라미터를 적용하여 다시 학습을 진행해보겠습니다.  \n",
    "\n",
    "- 학습을 진행하는 도중 최고 성능의 모델을 저장하세요.\n",
    "\n",
    "- 하이퍼파라미터를 저장하기 이전의 성능 그래프와 비교해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bacce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e63bcc",
   "metadata": {},
   "source": [
    "3. 최고 성능 모델을 사용하여 시뮬레이션을 진행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc37ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
